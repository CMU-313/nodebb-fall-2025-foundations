# LLM Translation Integration Summary

## Overview

The translation system now supports **two modes**:

1. **Hardcoded Mode** (default): Fast, deterministic translations for testing
2. **LLM Mode** (Ollama): Real-time language detection and translation using AI

## How It Works

### 1. Post Created/Edited in NodeBB
When a user creates or edits a post, NodeBB calls the translation microservice:

```javascript
// In src/translate/index.js
const response = await fetch(`http://localhost:5000/?content=${encodeURIComponent(postContent)}`);
const {is_english, translated_content} = await response.json();
```

### 2. Translation Service Processes Request
The Flask service (`app.py`) receives the request and calls `translate_content()`:

```python
# In src/translator.py
def translate_content(content: str) -> tuple[bool, str]:
    # Check if LLM mode is enabled
    use_llm = os.getenv("USE_LLM", "false").lower() == "true"
    
    if use_llm:
        # Use Ollama for real translation
        return translate_with_llm(content)
    else:
        # Use hardcoded translations
        return translate_hardcoded(content)
```

### 3. LLM Translation Process (when enabled)

**Step 1: Language Detection**
```python
def get_language(post, client, model):
    # Ask LLM: "What language is this?"
    # Returns: "Spanish", "French", "English", etc.
```

**Step 2: Check if English**
```python
if language.lower() == "english":
    return True, content  # No translation needed
```

**Step 3: Translate to English**
```python
def get_translation(post, client, model):
    # Ask LLM: "Translate this to English"
    # Returns: Translated text
```

### 4. NodeBB Receives Response

```javascript
{
  "is_english": false,  // true if content was English
  "translated_content": "Hello world"  // English translation
}
```

### 5. UI Rendering Decision

```javascript
if (!is_english) {
  // Show "Click to view translation" button
  // Store translated_content for when button is clicked
} else {
  // No button needed - content is already in English
}
```

## File Structure

```
/workspaces/nodebb-fall-2025-foundations/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ translate/
â”‚   â”‚   â””â”€â”€ index.js          # NodeBB integration (calls Flask API)
â”‚   â””â”€â”€ translator.py          # Translation logic (LLM + hardcoded)
â”œâ”€â”€ app.py                     # Flask web service
â”œâ”€â”€ test/
â”‚   â””â”€â”€ unit/
â”‚       â””â”€â”€ test_translator.py # Test suite (11 tests)
â”œâ”€â”€ pyproject.toml             # Python dependencies (includes ollama)
â”œâ”€â”€ Dockerfile.translator      # Docker deployment
â”œâ”€â”€ setup-ollama.sh            # Ollama setup script
â”œâ”€â”€ test_llm_translation.py    # Manual testing script
â””â”€â”€ TRANSLATOR_README.md       # Detailed documentation
```

## Quick Start

### Testing Mode (No LLM Required)
```bash
# Install dependencies
uv sync

# Run tests
uv run pytest -v

# Start Flask service
uv run flask run

# Test from NodeBB
# Posts in hardcoded languages will be translated
# Other posts will be treated as English
```

### LLM Mode (Requires Ollama)
```bash
# 1. Install Ollama (on host machine or accessible server)
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Start Ollama
ollama serve

# 3. Pull a model
ollama pull llama3.1:8b

# 4. Enable LLM mode and start Flask
USE_LLM=true OLLAMA_MODEL=llama3.1:8b uv run flask run

# Now all posts will use LLM for translation!
```

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `USE_LLM` | `false` | Enable Ollama LLM translation |
| `OLLAMA_HOST` | `http://localhost:11434` | Ollama server URL |
| `OLLAMA_MODEL` | `llama3.1:8b` | Model to use |
| `PORT` | `5000` (dev) / `8080` (prod) | Flask port |

## Integration Flow Diagram

```
User Creates/Edits Post
    â†“
NodeBB: src/translate/index.js
    â†“ HTTP Request
Flask: app.py (GET /?content=...)
    â†“
Python: src/translator.py
    â†“
â”Œâ”€ USE_LLM=false â”€â”         â”Œâ”€ USE_LLM=true â”€â”€â”
â”‚ Hardcoded Dict  â”‚         â”‚ Ollama LLM      â”‚
â”‚ - Fast          â”‚         â”‚ 1. Detect lang  â”‚
â”‚ - Deterministic â”‚         â”‚ 2. Translate    â”‚
â”‚ - For testing   â”‚         â”‚ - Real-time     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ - Any language  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                              â†“
Return: (is_english, translated_content)
    â†“
NodeBB receives response
    â†“
â”Œâ”€ is_english=true â”€â”      â”Œâ”€ is_english=false â”€â”
â”‚ No button shown   â”‚      â”‚ Show translate btn â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ Store translation  â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Example Usage

### Example 1: English Post (No Translation Needed)
```
Input: "Hello, how are you?"
  â†“ translate_content()
Output: (True, "Hello, how are you?")
  â†“ NodeBB
Result: No button shown
```

### Example 2: Spanish Post (Translation Shown)
```
Input: "Hola, Â¿cÃ³mo estÃ¡s?"
  â†“ translate_content()
  â†“ LLM detects: "Spanish"
  â†“ LLM translates
Output: (False, "Hello, how are you?")
  â†“ NodeBB
Result: Show "View translation" button
  â†“ User clicks button
Display: "Hello, how are you?"
```

## Testing

```bash
# Run all tests (hardcoded mode)
uv run pytest -v

# Test specific language
uv run pytest test/unit/test_translator.py::test_chinese -v

# Manual testing
uv run python test_llm_translation.py

# Test with LLM (requires Ollama)
USE_LLM=true uv run python test_llm_translation.py
```

## Next Steps

1. âœ… **Completed**: LLM integration with Ollama
2. âœ… **Completed**: Hardcoded fallback for testing
3. âœ… **Completed**: Flask API endpoints
4. âœ… **Completed**: NodeBB integration code
5. ğŸ”„ **Remaining**: Deploy Ollama in production
6. ğŸ”„ **Remaining**: Add caching layer (Redis)
7. ğŸ”„ **Remaining**: Add UI button in NodeBB templates
8. ğŸ”„ **Remaining**: Add performance monitoring

## Benefits of This Approach

âœ… **Flexible**: Switch between hardcoded and LLM modes
âœ… **Testable**: All tests pass without requiring Ollama
âœ… **Production-Ready**: LLM mode works with real AI models
âœ… **Fast Development**: Hardcoded mode for rapid iteration
âœ… **Scalable**: Microservice architecture allows independent scaling
âœ… **Maintainable**: Clear separation of concerns

## Troubleshooting

**Q: Tests are failing?**
A: Make sure `USE_LLM=false` (or unset). Tests use hardcoded translations.

**Q: LLM not working?**
A: Check:
1. Ollama is running: `ollama serve`
2. Model is pulled: `ollama list`
3. `USE_LLM=true` is set
4. `OLLAMA_HOST` is correct

**Q: Flask not starting?**
A: Run `uv sync` to install dependencies, check port 5000 is free.

**Q: How do I change models?**
A: Set `OLLAMA_MODEL=mistral:7b` (or any model from `ollama list`)
